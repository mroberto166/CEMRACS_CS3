{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', titlesize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let $\\mathcal{X}=\\mathcal{X}(D,\\mathbb{R}^{n_\\mathcal{X}})$ and $\\mathcal{Y}=\\mathcal{Y}(D,\\mathbb{R}^{n_\\mathcal{Y}})$ be two Banach spaces, with $D\\subset\\mathbb{R}$ being a bounded domain and $n_\\mathcal{X},n_\\mathcal{Y}\\in\\mathbb{N}$.\n",
    "We refer to the space $\\mathcal{X}$ as the input space (e.g. space of initial conditions) and the space $\\mathcal{Y}$ as the output space (e.g solution space of the PDE at time $T$).\n",
    "\n",
    "Let $\\mathcal{G}:\\mathcal{X}\\mapsto \\mathcal{Y}$ be an operator that represents the solution operator of a PDE of interest.\n",
    "The goal of this computer session is to approximate the operator $\\mathcal{G}$ with the surrogate $\\mathcal{N}:\\mathcal{X}\\to\\mathcal{Y}$ from finite data of measurements of input and output function pairs\n",
    "\n",
    "$$\\{\\bar{u}_{j}(x), u_j(x)\\}_{j=1}^M$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Moreover, we will consider the following PDE in the time-space domain $D_T = [0,1]\\times[-1,1]$:\n",
    "$$\n",
    "u_t + 5(u^3 - u) = \\epsilon u_{xx}, \\quad \\forall (t,x)\\in D_T\n",
    "$$\n",
    "with $\\epsilon=0.01$. The equation is known with the name of Allen-Cahn equation. The dynamic of the system evolves in time from the initial condition $u(0,x)=\\bar{u}(x)$, $x\\in[-1,1]$, and $\\bar{u}\\in\\mathcal{X}$.\n",
    "We are interested in approximating the operator\n",
    "$$\n",
    "\\mathcal{G}: \\mathcal{X} \\to \\mathcal{Y}, \\quad \\mathcal{G}: \\bar{u}(x) \\mapsto u_T(x)=u(1,x)\n",
    "$$\n",
    "which maps the initial datum to the solution of the PDE at time $t=T=1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data will download all the files required for the computer session. You must have 'wget' installed for the downloads to work.\n",
      "Folder Exists! Downloading there\n",
      "Downloading file AC_data_input_new.npy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-07-18 16:46:16--  https://zenodo.org/record/8114457/files/AC_data_input_new.npy\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16000128 (15M) [application/octet-stream]\n",
      "Saving to: ‘data/AC_data_input_new.npy.3’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 2.16M 7s\n",
      "    50K .......... .......... .......... .......... ..........  0% 4.98M 5s\n",
      "   100K .......... .......... .......... .......... ..........  0% 2.62M 5s\n",
      "   150K .......... .......... .......... .......... ..........  1% 5.77M 5s\n",
      "   200K .......... .......... .......... .......... ..........  1% 1.51M 6s\n",
      "   250K .......... .......... .......... .......... ..........  1% 1.85M 6s\n",
      "   300K .......... .......... .......... .......... ..........  2% 23.3M 5s\n",
      "   350K .......... .......... .......... .......... ..........  2% 1.60M 6s\n",
      "   400K .......... .......... .......... .......... ..........  2% 1.35M 6s\n",
      "   450K .......... .......... .......... .......... ..........  3% 4.00M 6s\n",
      "   500K .......... .......... .......... .......... ..........  3% 1.76M 6s\n",
      "   550K .......... .......... .......... .......... ..........  3% 2.01M 6s\n",
      "   600K .......... .......... .......... .......... ..........  4% 15.4M 6s\n",
      "   650K .......... .......... .......... .......... ..........  4% 2.61M 6s\n",
      "   700K .......... .......... .......... .......... ..........  4%  110M 5s\n",
      "   750K .......... .......... .......... .......... ..........  5% 2.59M 5s\n",
      "   800K .......... .......... .......... .......... ..........  5% 8.60M 5s\n",
      "   850K .......... .......... .......... .......... ..........  5% 3.08M 5s\n",
      "   900K .......... .......... .......... .......... ..........  6% 14.1M 5s\n",
      "   950K .......... .......... .......... .......... ..........  6% 1.03M 5s\n",
      "  1000K .......... .......... .......... .......... ..........  6% 98.8M 5s\n",
      "  1050K .......... .......... .......... .......... ..........  7% 9.53M 5s\n",
      "  1100K .......... .......... .......... .......... ..........  7%  553K 6s\n",
      "  1150K .......... .......... .......... .......... ..........  7% 3.97M 6s\n",
      "  1200K .......... .......... .......... .......... ..........  7%  831K 6s\n",
      "  1250K .......... .......... .......... .......... ..........  8% 12.0M 6s\n",
      "  1300K .......... .......... .......... .......... ..........  8% 3.57M 6s\n",
      "  1350K .......... .......... .......... .......... ..........  8%  723K 6s\n",
      "  1400K .......... .......... .......... .......... ..........  9% 3.55M 6s\n",
      "  1450K .......... .......... .......... .......... ..........  9% 37.0M 6s\n",
      "  1500K .......... .......... .......... .......... ..........  9%  508K 7s\n",
      "  1550K .......... .......... .......... .......... .......... 10% 3.15M 7s\n",
      "  1600K .......... .......... .......... .......... .......... 10%  557K 7s\n",
      "  1650K .......... .......... .......... .......... .......... 10% 2.81M 7s\n",
      "  1700K .......... .......... ......--2023-07-18 16:46:18--  https://zenodo.org/record/8114457/files/AC_data_output_new.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file AC_data_output_new.npy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8000128 (7.6M) [application/octet-stream]\n",
      "Saving to: ‘data/AC_data_output_new.npy.3’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 3.45M 2s\n",
      "    50K .......... .......... .......... .......... ..........  1% 6.56M 2s\n",
      "   100K .......... .......... .......... .......... ..........  1% 5.03M 2s\n",
      "   150K .......... .......... .......... .......... ..........  2% 13.3M 1s\n",
      "   200K .......... .......... .......... .......... ..........  3% 4.09M 1s\n",
      "   250K .......... .......... .......... .......... ..........  3% 28.6M 1s\n",
      "   300K .......... .......... .......... .......... ..........  4% 4.24M 1s\n",
      "   350K .......... .......... .......... .......... ..........  5% 4.23M 1s\n",
      "   400K .......... .......... .......... .......... ..........  5% 8.90M 1s\n",
      "   450K .......... .......... .......... .......... ..........  6% 3.92M 1s\n",
      "   500K .......... .......... .......... .......... ..........  7% 8.45M 1s\n",
      "   550K .......... .......... .......... .......... ..........  7% 7.16M 1s\n",
      "   600K .......... .......... .......... .......... ..........  8% 6.94M 1s\n",
      "   650K .......... .......... .......... .......... ..........  8% 7.21M 1s\n",
      "   700K .......... .......... .......... .......... ..........  9% 5.32M 1s\n",
      "   750K .......... .......... .......... .......... .......... 10% 4.54M 1s\n",
      "   800K .......... .......... .......... .......... .......... 10% 4.35M 1s\n",
      "   850K .......... .......... .......... .......... .......... 11% 25.1M 1s\n",
      "   900K .......... .......... .......... .......... .......... 12% 4.63M 1s\n",
      "   950K .......... .......... .......... .......... .......... 12% 4.94M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 13% 10.5M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 14% 4.42M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 14% 22.4M 1s\n",
      "  1150K .......... .......... .......... .......... ........"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"This data will download all the files required for the computer session.\"\n",
    "      \" You must have 'wget' installed for the downloads to work.\")\n",
    "\n",
    "folder_name = \"data\"\n",
    "if os.path.isdir(folder_name):\n",
    "    print(\"Folder Exists! Downloading there\")\n",
    "else:\n",
    "    print(\"Creating folder data\")\n",
    "    os.mkdir(folder_name)\n",
    "fnames = [\"AC_data_input_new.npy\", \"AC_data_output_new.npy\"]\n",
    "for fid, fname in enumerate(fnames):\n",
    "    print('Downloading file ' + fname + ':')\n",
    "    url = \"https://zenodo.org/record/8114457/files/\" + fname\n",
    "    cmd = f\"wget --directory-prefix {folder_name} {url}\"\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You are given with a set of input-output functions $\\{\\bar{u}_{j}(x), u_j(x)\\}_{j=1}^M$, stored in the data files \"AC_data_input_new.npy\" and \"AC_data_input_new.npy\", respectively. Specifically, the file \"AC_data_input_new.npy\" contains a Numpy array of shape $M\\times P\\times 2$ ($M=1000$, $P=1000$),\n",
    "\n",
    "$$\n",
    "\\text{input_data}[j, i, 0] = x_i, \\quad \\forall j=1,...,M\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{input_data}[j, i, 1] = \\bar{u}_{j}(x_i)\n",
    "$$\n",
    "\n",
    "On the other hand, the file \"AC_data_input_new.npy\" contains a Numpy array of shape $M\\times P$ ($M=1000$, $P=1000$):\n",
    "\n",
    "$$\n",
    "\\text{output_data}[j, i] = u_{j}(x_i)\n",
    "$$\n",
    "\n",
    "The data contained in the file will represent our \"continuous\" signal. We say continuous because it is sampled at a very fine grid ($P=1000$).\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/data.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input_data and output_data contain all the input and output functions evaluated on a grid with 1000 points\n",
    "input_data = torch.from_numpy(np.load(\"data/AC_data_input_new.npy\")).type(torch.float32)  # Shape (1000, 1000, 2)\n",
    "output_data = torch.from_numpy(np.load(\"data/AC_data_output_new.npy\")).type(torch.float32)  # Shape (1000, 1000)\n",
    "\n",
    "# input_data[:, :, 0] contains the values of the x coordinates where the input functions are evaluated\n",
    "# input_data[:, :, 1] contains the values of the input functions at the given x coordinates\n",
    "# output_data contains the values of the output functions at the given x coordinates (same as the coordinates where the input functions are evaluated)\n",
    "\n",
    "# Plot 4 samples from the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(30, 10), dpi=100)\n",
    "axes[0, 0].grid(True, which=\"both\", ls=\":\")\n",
    "axes[0, 1].grid(True, which=\"both\", ls=\":\")\n",
    "axes[1, 0].grid(True, which=\"both\", ls=\":\")\n",
    "axes[1, 1].grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "axes[0, 0].scatter(input_data[0, :, 0], input_data[0, :, 1], label=r'$u_0(x)$')\n",
    "axes[0, 0].scatter(input_data[0, :, 0], output_data[0, :], label=r'$u_T(x)$')\n",
    "\n",
    "axes[0, 1].scatter(input_data[0, :, 0], input_data[1, :, 1], label=r'$u_0(x)$')\n",
    "axes[0, 1].scatter(input_data[0, :, 0], output_data[0, :], label=r'$u_T(x)$')\n",
    "\n",
    "axes[1, 0].scatter(input_data[0, :, 0], input_data[2, :, 1], label=r'$u_0(x)$')\n",
    "axes[1, 0].scatter(input_data[0, :, 0], output_data[0, :], label=r'$u_T(x)$')\n",
    "\n",
    "axes[1, 1].scatter(input_data[0, :, 0], input_data[3, :, 1], label=r'$u_0(x)$')\n",
    "axes[1, 1].scatter(input_data[0, :, 0], output_data[0, :], label=r'$u_T(x)$')\n",
    "\n",
    "axes[0, 0].set(title=\"Sample 1\")\n",
    "axes[0, 1].set(title=\"Sample 2\")\n",
    "axes[1, 0].set(xlabel=r'$x$', title=\"Sample 3\")\n",
    "axes[1, 1].set(xlabel=r'$x$', title=\"Sample 4\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 1].legend()\n",
    "axes[1, 0].legend()\n",
    "axes[1, 1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate the output functions to the coordinates x, so that we have a tensor with the same shape as the input\n",
    "output_data = torch.cat((input_data[:, :, 0].unsqueeze(-1), output_data.unsqueeze(-1)), -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first step to construct an approximate operator $\\mathcal{G}^\\ast$ is to suitably *encode* the input and output functions (to a lower dimensional space), in order to be able to process them with a computer.\n",
    "Le us define the input and output *encoding operator*\n",
    "$$\n",
    "     \\mathcal{E}:\\mathcal{Z}\\mapsto\\mathbb{R}^m,\n",
    "$$\n",
    "where $\\mathcal{Z}$ is a given function space (for instance the input or the output function spaces $\\mathcal{X}$, $\\mathcal{Y}$) and, $m$ is the chosen *encoding dimensions*. In this case $m\\ll P$, where $P=1000$ represents our \"continuous\" sampling rate.\n",
    "\n",
    "#### Exercises\n",
    "- Implement the grid_sensor_encoding member of the class Encoding which evaluate the function $f$ at given sensor points: $\\mathcal{E}(f) = (f(x_1),\\dots,f(x_N))$, where $x_1, x_2, ..., x_N$ are *random* points\n",
    "- Implement the grid_sensor_encoding member of the class Encoding which evaluate the function $f$ at given sensor points: $\\mathcal{E}(f) = (f(x_1),\\dots,f(x_N))$, where $x_1, x_2, ..., x_N$ are *equispaced* points\n",
    "- Implement the fourier_encoding member of the class Encoding which compute the Fourier coefficients of the function $f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoding(nn.Module):\n",
    "\n",
    "    def __init__(self, encoding_dimension):\n",
    "        super(Encoding, self).__init__()\n",
    "        '''\n",
    "        :param encoding_dimensions: encoding dimension m\n",
    "        '''\n",
    "        self.encoding_dimension = encoding_dimension\n",
    "\n",
    "    def random_sensor_encoding(self, functions):\n",
    "        '''\n",
    "        :param functions: this tensor represents a batch of size M of the continuous functions $\\{u_{j}(x)\\}_{j=1}^M$.\n",
    "        On a computer we represent a continuous function through a highly sampled discrete signal.\n",
    "        Therefore, functions is a tensor of shape MxPx2, where P=1000 is the length of the highly-sampled function,\n",
    "        and functions[j, i, 0] = x_i and functions[j, i, 1] = u_{j}(x_i),\n",
    "        with x_1, x_2,...,x_i,...,x_P being the coordinates where the functions are sampled\n",
    "        :return: encoding_coordinates, encoded_functions.\n",
    "        encoding_coordinates: a tensor of shape (m,), (with m=encoding_dimension), corresponding to the discrete input coordinates, encoding_coordinates[i] = x_i, (in this case the random sensor points coordinates).\n",
    "        encoded_functions: tensors of shape Mxm, corresponding to the discrete encoded functions encoded_functions[j,i]=u_{j}(x_i).\n",
    "        '''\n",
    "        M = functions.shape[0]\n",
    "        P = functions.shape[1]\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # TODO: implement grid encoding\n",
    "        encoding_coordinates = torch.zeros((m,))\n",
    "        encoded_functions = torch.zeros((M, m))\n",
    "        return encoding_coordinates, encoded_functions\n",
    "\n",
    "    def grid_sensor_encoding(self, functions):\n",
    "        '''\n",
    "        :param functions: this tensor represents a batch of size M of the continuous functions $\\{u_{j}(x)\\}_{j=1}^M$.\n",
    "        On a computer we represent a continuous function through a highly sampled discrete signal.\n",
    "        Therefore, functions is a tensor of shape MxPx2, where P=1000 is the length of the highly-sampled function,\n",
    "        and functions[j, i, 0] = x_i and functions[j, i, 1] = u_{j}(x_i),\n",
    "        with x_1, x_2,...,x_i,...,x_P being the coordinates where the functions are sampled\n",
    "        :return: encoding_coordinates, encoded_functions.\n",
    "        encoding_coordinates: a tensor of shape (m,), (with m=encoding_dimension), corresponding to the discrete input coordinates, encoding_coordinates[i] = x_i, (in this case the equispaced sensor points coordinates).\n",
    "        encoded_functions: tensors of shape Mxm, corresponding to the discrete encoded functions encoded_functions[j,i]=u_{j}(x_i).\n",
    "        '''\n",
    "        M = functions.shape[0]\n",
    "        P = functions.shape[1]\n",
    "        # TODO: implement grid encoding\n",
    "        encoding_coordinates = torch.zeros((m,))\n",
    "        encoded_functions = torch.zeros((M, m))\n",
    "        return encoding_coordinates, encoded_functions\n",
    "\n",
    "    def fourier_encoding(self, functions):\n",
    "        '''\n",
    "        :param functions: this tensor represents a batch of size M of the continuous functions $\\{u_{j}(x)\\}_{j=1}^M$.\n",
    "        On a computer we represent a continuous function through a highly sampled discrete signal.\n",
    "        Therefore, functions is a tensor of shape MxPx2, where P=1000 is the length of the highly-sampled function,\n",
    "        and functions[j, i, 0] = x_i and functions[j, i, 1] = u_{j}(x_i),\n",
    "        with x_1, x_2,...,x_i,...,x_P being the coordinates where the functions are sampled\n",
    "        :return: encoding_coordinates, encoded_functions.\n",
    "        encoding_coordinates: a tensor of shape m, (with m=encoding_dimension), corresponding to the discrete input coordinates, encoding_coordinates[i] = f_i, (in this case the first m frequencies).\n",
    "        encoded_functions: tensors of shape MxN, corresponding to the first $m$ Fourier coefficients at the encoded_functions[j,i,0]=Real(F[u_{j}](f_i)), encoded_functions[j,i,1]=Imag(F[u_{j}](f_i)) where F is the discrete FT.\n",
    "        '''\n",
    "        M = functions.shape[0]\n",
    "        P = functions.shape[1]\n",
    "        # Nyquist frequency (max resolvable frequency)\n",
    "        f_nyquist = P // 2\n",
    "\n",
    "        # Use https://pytorch.org/docs/stable/generated/torch.fft.rfft.html#torch.fft.rfft to compute the Fourier coefficients\n",
    "        # TODO: implement Fourier encoding\n",
    "        frequencies = torch.zeros((m,))\n",
    "        encoded_functions = torch.zeros((M, m, 2))\n",
    "\n",
    "        return frequencies, encoded_functions\n",
    "\n",
    "    def forward(self, functions, which):\n",
    "        if which == \"Random\":\n",
    "            return self.random_sensor_encoding(functions)\n",
    "        elif which == \"Grid\":\n",
    "            return self.grid_sensor_encoding(functions)\n",
    "        elif which == \"Fourier\":\n",
    "            return self.fourier_encoding(functions)\n",
    "        else:\n",
    "            raise ValueError(r'which must be one among \"Random\", \"Grid\", and \"Fourier\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the implementation of the encoding by plotting the encoded inputs and outputs\n",
    "m = 50  # This is the encoding dimension\n",
    "encoding_operator = Encoding(encoding_dimension=m)\n",
    "\n",
    "for which_enconder in [\"Random\", \"Grid\", \"Fourier\"]:\n",
    "    encoding_coordinates_input, encoding_function_input = encoding_operator(input_data, which_enconder)\n",
    "    encoding_coordinates_output, encoding_function_output = encoding_operator(output_data, which_enconder)\n",
    "    # Set of coordinates where input functions are evaluated (variable x in the notes)\n",
    "\n",
    "    if which_enconder != \"Fourier\":\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(30, 12))\n",
    "        axes[0, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[0, 1].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[1, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[1, 1].grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "        axes[0, 0].scatter(encoding_coordinates_input, encoding_function_input[0], label=r'$u_0(x)$')\n",
    "        axes[0, 0].scatter(encoding_coordinates_input, encoding_function_output[0], label=r'$u_T(x)$')\n",
    "\n",
    "        axes[0, 1].scatter(encoding_coordinates_input, encoding_function_input[1], label=r'$u_0(x)$')\n",
    "        axes[0, 1].scatter(encoding_coordinates_input, encoding_function_output[1], label=r'$u_T(x)$')\n",
    "\n",
    "        axes[1, 0].scatter(encoding_coordinates_input, encoding_function_input[2], label=r'$u_0(x)$')\n",
    "        axes[1, 0].scatter(encoding_coordinates_input, encoding_function_output[2], label=r'$u_T(x)$')\n",
    "\n",
    "        axes[1, 1].scatter(encoding_coordinates_input, encoding_function_input[3], label=r'$u_0(x)$')\n",
    "        axes[1, 1].scatter(encoding_coordinates_input, encoding_function_output[3], label=r'$u_T(x)$')\n",
    "\n",
    "        axes[0, 0].set(title=which_enconder + \" Encoder, Sample 1\")\n",
    "        axes[0, 1].set(title=which_enconder + \" Encoder, Sample 2\")\n",
    "\n",
    "        axes[1, 0].set(xlabel=\"$x$\", title=which_enconder + \" Encoder,  Sample 3\")\n",
    "        axes[1, 1].set(xlabel=\"$x$\", title=which_enconder + \" Encoder,  Sample 4\")\n",
    "\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 1].legend()\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 1].legend()\n",
    "    else:\n",
    "        fig, axes = plt.subplots(4, 2, figsize=(30, 18))\n",
    "        axes[0, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[0, 1].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[1, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[1, 1].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[2, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[2, 1].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[3, 0].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[3, 1].grid(True, which=\"both\", ls=\":\")\n",
    "        axes[0, 0].scatter(encoding_coordinates_input, encoding_function_input[0, :, 0], label=r'${Real}(u_0(x))$')\n",
    "        axes[0, 0].scatter(encoding_coordinates_input, encoding_function_output[0, :, 0], label=r'${real}(u_T(x))$')\n",
    "\n",
    "        axes[0, 1].scatter(encoding_coordinates_input, encoding_function_input[0, :, 1], label=r'${Imag}(u_0(x))$')\n",
    "        axes[0, 1].scatter(encoding_coordinates_input, encoding_function_output[0, :, 1], label=r'${Imag}(u_T(x))$')\n",
    "\n",
    "        axes[1, 0].scatter(encoding_coordinates_input, encoding_function_input[1, :, 0], label=r'${Real}(u_0(x))$')\n",
    "        axes[1, 0].scatter(encoding_coordinates_input, encoding_function_output[1, :, 0], label=r'${real}(u_T(x))$')\n",
    "\n",
    "        axes[1, 1].scatter(encoding_coordinates_input, encoding_function_input[1, :, 1], label=r'${Imag}(u_0(x))$')\n",
    "        axes[1, 1].scatter(encoding_coordinates_input, encoding_function_output[1, :, 1], label=r'${Imag}(u_T(x))$')\n",
    "\n",
    "        axes[2, 0].scatter(encoding_coordinates_input, encoding_function_input[2, :, 0], label=r'${Real}(u_0(x))$')\n",
    "        axes[2, 0].scatter(encoding_coordinates_input, encoding_function_output[2, :, 0], label=r'${real}(u_T(x))$')\n",
    "\n",
    "        axes[2, 1].scatter(encoding_coordinates_input, encoding_function_input[2, :, 1], label=r'${Imag}(u_0(x))$')\n",
    "        axes[2, 1].scatter(encoding_coordinates_input, encoding_function_output[2, :, 1], label=r'${Imag}(u_T(x))$')\n",
    "\n",
    "        axes[3, 0].scatter(encoding_coordinates_input, encoding_function_input[3, :, 0], label=r'${Real}(u_0(x))$')\n",
    "        axes[3, 0].scatter(encoding_coordinates_input, encoding_function_output[3, :, 0], label=r'${real}(u_T(x))$')\n",
    "\n",
    "        axes[3, 1].scatter(encoding_coordinates_input, encoding_function_input[3, :, 1], label=r'${Imag}(u_0(x))$')\n",
    "        axes[3, 1].scatter(encoding_coordinates_input, encoding_function_output[3, :, 1], label=r'${Imag}(u_T(x))$')\n",
    "\n",
    "        axes[0, 0].set(title=which_enconder + \" Encoder, Real Part Sample 1\")\n",
    "        axes[0, 1].set(title=which_enconder + \" Encoder, Imaginary Part Sample 1\")\n",
    "\n",
    "        axes[1, 0].set(title=which_enconder + \" Encoder, Real Part Sample 2\")\n",
    "        axes[1, 1].set(title=which_enconder + \" Encoder, Imaginary Part Sample 2\")\n",
    "\n",
    "        axes[2, 0].set(title=which_enconder + \" Encoder, Real PartSample 3\")\n",
    "        axes[2, 1].set(title=which_enconder + \" Encoder, Imaginary Part Sample 3\")\n",
    "\n",
    "        axes[3, 0].set(xlabel=\"Frequency\", title=which_enconder + \" Encoder, Real Part Sample 4\")\n",
    "        axes[3, 1].set(xlabel=\"Frequency\", title=which_enconder + \" Encoder, Imaginary Part Sample 4\")\n",
    "\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 1].legend()\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 1].legend()\n",
    "\n",
    "        axes[3, 0].legend()\n",
    "        axes[3, 1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multi Layer Perceptron\n",
    "Given an input $x \\in D \\subset R^n$, a feedforward neural network (or Multi Layer Perceptron) transforms it to an output $u_\\theta(x)\\in R^m$, through a layer of units (neurons) which compose of either affine-linear maps between units (in successive layers) or scalar non-linear activation functions within units, resulting in the representation,\n",
    "\n",
    "$$u_{\\theta}(x) = C_K \\circ \\sigma \\circ C_{K-1}\\ldots \\ldots \\ldots \\circ \\sigma \\circ C_2 \\circ \\sigma \\circ C_1(x).$$\n",
    "\n",
    "Here, $\\circ$ refers to the composition of functions and $\\sigma $ is a scalar (non-linear) activation function. For any $1 \\leq k \\leq K$, we define\n",
    "\n",
    "$$\n",
    "C_k z_k = W_k z_k + b_k, \\quad {\\rm for} ~ W_k \\in R^{d_{k+1} \\times d_k}, z_k \\in R^{d_k}, b_k \\in R^{d_{k+1}}.\n",
    "$$\n",
    "\n",
    "We also denote, \n",
    "$$\n",
    "\\theta = \\{W_k, b_k\\}, \\quad \\forall~ 1 \\leq k \\leq K,\n",
    "$$\n",
    "to be the concatenated set of (tunable) weights for our network.\n",
    "\n",
    "We define:\n",
    "- $K$: number of hidden layers\n",
    "- $d_{k+1}=d_{k}=\\bar{d}$, for all $k=1,...,K$: number of neurons\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/ffnn.png\" width=\"500\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers_=4, neurons_=20, retrain_seed_=42):\n",
    "        super(MLP, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons_\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers_\n",
    "        # Activation function\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed_\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DeepONet\n",
    "\n",
    "The first operator learning architecture we are gping to see is *DeepONet*, which belongs to the class of models known as *operator networks*.\n",
    "Let $D \\subset \\mathbb{R}, D' \\subset \\mathbb{R}$, and $\\mathcal{X} = \\mathcal{X}(D, \\mathbb{R})$ and $\\mathcal{Y} = \\mathcal{Y}(D', \\mathbb{R})$ be suitable function spaces. Then, a *DeepONet* is the operator, $\\mathcal{N}^{DON}: \\mathcal{X} \\to \\mathcal{Y}$, given by\n",
    "\n",
    "$$\n",
    "\\mathcal{N}^{DON}(\\bar{u})(y) = \\sum_{k=1}^p \\beta_k(\\bar{u}) \\tau_k(y),\n",
    "\\quad \\bar{u} \\in \\mathcal{X}, \\; y \\in D',\n",
    "$$\n",
    "\n",
    "where the *branch-net* ${\\beta}$ is a neural network that maps an encoding $\\mathcal{E}(\\bar{u})$ of the input function $\\bar{u}$ to $\\mathbb{R}^p$:\n",
    "\n",
    "$$\n",
    "{\\beta}: \\mathbb{R}^m \\to \\mathbb{R}^p, ~\\mathcal{E}(\\bar{u}) \\mapsto (\\beta_1(\\mathcal{E}(\\bar{u})),\\dots, \\beta_p(\\mathcal{E}(\\bar{u})),\n",
    "$$\n",
    "\n",
    "and the *trunk-net* ${\\tau}(y) = (\\tau_1(y),\\dots, \\tau_p(y))$ is another neural network mapping,\n",
    "\n",
    "$$\n",
    "{\\tau}: D' \\to \\mathbb{R}^p, \\quad y\\mapsto (\\tau_1(y),\\dots, \\tau_p(y)).\n",
    "$$\n",
    "\n",
    "Thus, a DeepONet combines the branch net (as coefficient functions) and trunk net (as basis functions) to create a mapping between functions.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/dona.png\" width=\"700\" >\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/donetb.png\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We assume that both the branch-net and trunk-net are MLP. The dataset for DeepONet is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{S}^{DON} = \\{ y, ~ \\mathcal{E}(\\bar{u}_{j}), ~u_j(y), ~ j=1,...,M\\}, \\quad y\\in D'= D\n",
    "$$\n",
    "\n",
    "We will assume for all the experiments  that $y=\\{y_1, y_2, ..., y_N\\}$ are equi-spaced grid points: $y_i -{y_{i-1}}=\\Delta y$, $\\forall i=1,...,N$\n",
    "\n",
    "Let us start by consider encoding of the input functions at randomly distributed sensor points:\n",
    "\n",
    "$$\n",
    "\\mathcal{E}(\\bar{u}_{j}) = (\\bar{u}(x_1),\\dots,\\bar{u}(x_m))\n",
    "$$\n",
    "with $\\{ x_1, x_2, ..., x_m \\}$ uniformly distributed random points.\n",
    "\n",
    "In PyTorch $y$ is tensor of shape (N,1), the encoded input functions $\\mathcal{E}(\\bar{u}_{j}), ~ j=1,...,M$ are collected in a tensor ($M\\times m$) and the output function $u_j(y), ~ j=1,...,M$ in a tensor of shape ($M\\times N\n",
    "$). Keep in mind that $N$ and $m$ can be different. However, we will consider the same encoding dimensions $m=N$ of inputs and outputs.\n",
    "\n",
    "Observe that we construct batches only with respect to the function samples. Instead, during the training we give as input to the trunk the *entire* tensor $y$ at every training step!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercises\n",
    "- Implement  the member forward of the class DeepONet. The function, takes as input u_, which is fed to the branch-net, and y_ which is instead fed to the trunk-net. u_ has shape (B, m), with $B\\leq M$, being the batch size, and y_ a tensor of shape (N,1), corresponding to $\\textbf{y}=\\{y_1, y_2, ..., y_N\\}$. The output has a shape (B, N).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepOnet(nn.Module):\n",
    "    def __init__(self, branch, trunk):\n",
    "        super(DeepOnet, self).__init__()\n",
    "        self.branch = branch\n",
    "        self.trunk = trunk\n",
    "\n",
    "    def forward(self, u_, y_):\n",
    "        # TODO: Implement the forward function\n",
    "        # u_ has shape (B, m)\n",
    "        # y_ ha shape (N,1), N=m\n",
    "        B = u_.shape[0]\n",
    "        N = y_.shape[0]\n",
    "\n",
    "        output = torch.zeros((B, N))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DeepONet with Random Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Using encoding at random sensors for the input functions\")\n",
    "# Encoding coordinates are not needed by DeepONet\n",
    "_, random_encoding_input_donet = encoding_operator(input_data, \"Random\")\n",
    "print(\"Shape of the encoded input tensor: \", random_encoding_input_donet.shape)\n",
    "\n",
    "# TODO: encode the output functions at grid points\n",
    "# We assume that the output function is evaluated at grid points {y_1, y_2, ..., y_N}\n",
    "grid_encoding_coordinates_donet, grid_encoding_output = encoding_operator(output_data, \"Grid\")\n",
    "print(\"Shape of the the encoded output tensor: \", grid_encoding_output.shape)\n",
    "grid_encoding_coordinates_donet = grid_encoding_coordinates_donet.unsqueeze(-1)  # this is what called y\n",
    "print(\"Shape of the coordinates y where to evaluate the output functions: \", grid_encoding_coordinates_donet.shape)\n",
    "\n",
    "# Split train-testing data and create the corresponding data loaders for the training\n",
    "n_train = 500\n",
    "batch_size = 20\n",
    "\n",
    "random_encoding_input_donet_train = random_encoding_input_donet[:n_train, :]\n",
    "random_encoding_input_donet_test = random_encoding_input_donet[n_train:, :]\n",
    "\n",
    "grid_encoding_output_train = grid_encoding_output[:n_train, :]\n",
    "grid_encoding_output_test = grid_encoding_output[n_train:, :]\n",
    "\n",
    "# Define the dataloader\n",
    "random_training_set_don = DataLoader(TensorDataset(random_encoding_input_donet_train, grid_encoding_output_train), batch_size=batch_size, shuffle=True)\n",
    "random_testing_set_don = DataLoader(TensorDataset(random_encoding_input_donet_test, grid_encoding_output_test), batch_size=n_train, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "Define the trunk and the branch for a DeepONet with Random encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_hidden_layers = 4\n",
    "neurons = 50\n",
    "retrain_seed = 42\n",
    "\n",
    "p = 25  # number of basis functions\n",
    "\n",
    "#TODO: Define BranchNet and TrunkNet as MLPs\n",
    "\n",
    "branch_net = None\n",
    "trunk_net = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "donet_random = DeepOnet(branch_net, trunk_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DeepONet with Grid Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Using encoding at grid sensors for the input functions\")\n",
    "# TODO: encode the input functions at grid points\n",
    "grid_encoding_coordinates_don, grid_encoding_input_donet = encoding_operator(input_data, \"Grid\")\n",
    "print(\"Shape of the encoded input tensor: \", random_encoding_input_donet.shape)\n",
    "\n",
    "# Split train-testing data and create the corresponding data loaders for the training\n",
    "grid_encoding_input_donet_train = grid_encoding_input_donet[:n_train, :]\n",
    "grid_encoding_input_donet_test = grid_encoding_input_donet[n_train:, :]\n",
    "\n",
    "# Define the dataloader\n",
    "grid_training_set_don = DataLoader(TensorDataset(grid_encoding_input_donet_train, grid_encoding_output_train), batch_size=batch_size, shuffle=True)\n",
    "grid_testing_set_don = DataLoader(TensorDataset(grid_encoding_input_donet_test, grid_encoding_output_test), batch_size=n_train, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "Define the trunk and the branch for a DeepONet with Grid encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Define BranchNet and TrunkNet as MLPs\n",
    "branch_net = None\n",
    "trunk_net = None\n",
    "donet_grid = DeepOnet(branch_net, trunk_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DeepONet with Fourier Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Using encoding given by the Fourier coefficients of the input functions\")\n",
    "# TODO: encode the input functions at grid pints\n",
    "_, fourier_encoding_input_donet = encoding_operator(input_data, \"Fourier\")\n",
    "print(\"Shape of the encoded input tensor: \", fourier_encoding_input_donet.shape)\n",
    "# reshape the input from 1000xmx2 (real and imaginary part) to 1000x(2*m)\n",
    "fourier_encoding_input_donet = fourier_encoding_input_donet.reshape(fourier_encoding_input_donet.shape[0], 2 * m)\n",
    "print(\"Shape after reshaping of the encoded input tensor: \", fourier_encoding_input_donet.shape)\n",
    "\n",
    "# Split train-testing data and create the corresponding data loaders for the training\n",
    "fourier_encoding_input_donet_train = fourier_encoding_input_donet[:n_train, :]\n",
    "fourier_function_input_donet_test = fourier_encoding_input_donet[n_train:, :]\n",
    "\n",
    "# Define the dataloader\n",
    "fourier_training_set_don = DataLoader(TensorDataset(fourier_encoding_input_donet_train, grid_encoding_output_train), batch_size=batch_size, shuffle=True)\n",
    "fourier_testing_set_don = DataLoader(TensorDataset(fourier_function_input_donet_test, grid_encoding_output_test), batch_size=n_train, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "Define the trunk and the branch for a DeepONet with Fourier encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Define BranchNet and TrunkNet as MLPs\n",
    "branch_net = None\n",
    "trunk_net = None\n",
    "donet_fourier = DeepOnet(branch_net, trunk_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fourier Neural Operator\n",
    "Recall the general structure of a Neural Operator $\\mathcal{N}^{NO}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}^{NO}(\\bar{u})(x)  = Q \\circ \\mathcal{L}_T \\circ \\dots \\mathcal{L}_t \\circ \\dots \\circ \\mathcal{L}_1\\circ R(v_0)(x)\n",
    "$$\n",
    "\n",
    "where $R: \\mathbb{R}^{d_u} \\to  \\mathbb{R}^{d_v}, \\quad R: v_0 \\mapsto v_1$ is linear or nonlinear transformation of the encoded input, $\\mathcal{L}_t$, $t=1,...,T$ are integral layers and $Q:\\mathbb{R}^{d_v} \\to  \\mathbb{R}$ is a projection to the original space. The integral layer is generally defined as:\n",
    "\n",
    "$$\n",
    "v_{t+1}(x) = \\mathcal{L}_t(v_{t})(x) = \\sigma\\left(W_t v_t(x) + B_t(x) + \\mathcal{K}_t(v_t)(x) \\right), \\quad \\mathcal{K}(v_t)(x) = \\int_D \\kappa_{t}(x,y)v_t dy, \\quad \\forall x \\in D\n",
    "$$\n",
    "\n",
    "where $W_t\\in \\mathbb{R}^{d_v}\\times \\mathbb{R}^{d_v}$ is a *local linear operators* and $B_t:D \\to \\mathbb{R}^{d_v}$ is a *bias function*, and $\\kappa_{t}\\in C(D\\times D; \\mathbb{R}^{d_v}\\times\\mathbb{R}^{d_v})$ being a *kernel function*.\n",
    "\n",
    "Observe that in this case, we called the input $v_0$, and not $\\bar{u}$. This is not a mistake. In fact, in the original paper, the input is the concatenation of the coordinates $\\textbf{x} =\\{x_i\\}_{i=1}^m$ where the input is defined and the actual input function $\\{\\bar{u}(x_i)\\}_{i=1}^m$. So in this case, observe that $d_u=2$.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/fnoinp.png\" width=\"850\">\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "In the case of FNO, by letting $\\kappa_{t}(x,y) = \\kappa_{t}(x -y)$ and exploiting the convolution theorem,  the integral kernel can be explicitly formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{K}_t(v_t) = \\mathcal{F}^{-1} \\left[\\mathcal{F}[\\kappa_t](k) \\cdot \\mathcal{F} [v_t](k) \\right]\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}[v]$ represents the Fourier transform of a function $v$, and $\\mathcal{F}^{-1}[v]$ its inverse.\n",
    "\n",
    "In the discrete setting:\n",
    "- the Fourier transform is replaced by the Fourier coefficients $\\mathcal{F}_m [v_t](k)$ of the discrete Fourier transform (DFT) of the function $v_t(x)$, and we truncate the corresponding modes up to $k_{modes}$ (all the remaining modes are set to zero!). Since we use FFT algorithm to compute the Fourier coefficients, for FNO we consider only encoding at grid points $\\textbf{x} =\\{x_i\\}_{i=1}^m$.\n",
    "- the DFT  of $\\kappa_t$ at the frequency $k$, $\\mathcal{F}_m[\\kappa_t](k)$, is parameterized as complex-valued ($d_v \\times d_v$)-tensors $P_t(k)\\in\\mathbb{C}^{ d_v\\times d_v}$, for all $k=1,...,k_{modes}$.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"ImagesCEMRACS/FNO1.png\" width=\"750\">\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset for FNO is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{S}^{FNO} = \\{\\bar{u}_{j}(\\textbf{x}), ~u_j(\\textbf{x}), ~ j=1,...,M\\}, \\quad \\textbf{x} =\\{x_i\\}_{i=1}^m, ~\\text{being Cartesian grid points}\n",
    "$$\n",
    "\n",
    "Observe that since we use FFT algorithm we are constrained to grid points. We do not bother about concatenating the input functions to the coordinates to get the tensors $v_{0,j}$ beforehand. This is done in the forward function of FNO1d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Using encoding at grid sensors for the input functions\")\n",
    "grid_encoding_coordinates_fno, grid_encoding_input_fno = encoding_operator(input_data, \"Grid\")\n",
    "\n",
    "grid_encoding_input_fno_train = grid_encoding_input_fno[:n_train]\n",
    "grid_encoding_input_fno_test = grid_encoding_input_fno[n_train:]\n",
    "\n",
    "training_set_fno = DataLoader(TensorDataset(grid_encoding_input_fno_train, grid_encoding_output_train), batch_size=batch_size, shuffle=True)\n",
    "testing_set_fno = DataLoader(TensorDataset(grid_encoding_input_fno_test, grid_encoding_output_test), batch_size=n_train, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Structure and Implementation\n",
    "\n",
    "In terms of data structures:\n",
    "- the input to the linear function $R$ (first operation in FNO) is a tensor of shape $(M\\times m \\times 2)$, where the first channel corresponds to the coordinates $\\textbf{x}$ and the second to the input function $\\bar{u}(\\textbf{x})$. See picture above. The output $v_1$ is a tensor of shape $(M\\times m \\times d_v)$.\n",
    "- the functions $v_t$ are represented by tensors of shape $(M\\times d_v \\times m)$. In the code the last two axes of the tensor $v_1$, output of the transformation $R$ are permuted in order to compute the DFT: $(M\\times m \\times d_v) \\to (M\\times d_v\\times m)$. Only before applying the transformation $Q$, the axes are permuted back to $(M\\times m \\times d_v)$.\n",
    "- the DFT is computed through the function torch.fft.rfft and, applied to a tensor $(M\\times d_v\\times m)$ and returns a tensor of shape $(M\\times d_v\\times K)$, where $K$ is the total number of modes $k_{modes}<K$.\n",
    "- the output of FNO is a tensor of shape $M\\times m$\n",
    "- the ($d_v \\times d_v$)-complex-valued-tensors $P_t(k)\\in\\mathbb{C}^{ d_v\\times d_v}$, $k=1,...,k_{modes}$ parametrizing the kernel, are collected together to obtain a ($d_v \\times d_v \\times k_{modes}$)- complex-valued tensor (this is the variable **weights** in the class **IntegralKernel**)\n",
    "\n",
    "#### Exercises\n",
    "- Implement the forward member of the class IntegralKernel\n",
    "- Implement the integral_layer member of the class FNO1d\n",
    "- Complete the forward member of the class FNO1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class IntegralKernel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(IntegralKernel, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels  # This is d_v\n",
    "        self.out_channels = out_channels  # This is d_v\n",
    "        self.modes1 = modes1  # This is k_max\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "\n",
    "        # Parametrization of the kernel\n",
    "        self.weights = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]  # Batch size\n",
    "        dv = x.shape[1]  # Lifting-dimension\n",
    "        n = x.shape[-1]  # Number of grid points where input and intermediate states are evaluated\n",
    "\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        # TODO: Compute Fourier coefficients\n",
    "\n",
    "        # TODO: Use compl_mul1d to perform the multiplication between the relevant Fourier Modes and self.weights and fill the tensor out_tf with the corresponding values\n",
    "\n",
    "        # TODO: Return to physical space\n",
    "        x = torch.zeros((batchsize, dv, n))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 3 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.R .\n",
    "        2. 3 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.Q.\n",
    "\n",
    "        input: the solution of the initial condition and location (x, bar(u(x)))\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution at a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 0  # pad the domain if input is non-periodic\n",
    "\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        # Lifting transformation, corresponding to a simple linear transformation\n",
    "        self.R = nn.Linear(2, self.width)  # input channel is 2: (x, u0(x))\n",
    "\n",
    "        # Integral kernels of 3 sequential layers\n",
    "        self.integral_kernel_1 = IntegralKernel(self.width, self.width, self.modes1)\n",
    "        self.integral_kernel_2 = IntegralKernel(self.width, self.width, self.modes1)\n",
    "        self.integral_kernel_3 = IntegralKernel(self.width, self.width, self.modes1)\n",
    "\n",
    "        # Linear transformation of 3 sequential layers\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        # Projection layer, corresponding to a MLP with 32 neurons and 1 hidden layer\n",
    "        self.Q = nn.Sequential(nn.Linear(self.width, 32), self.activation, nn.Linear(32, 1))\n",
    "\n",
    "    def integral_layer(self, x, integral_kernel, linear_transformation):\n",
    "        # TODO: Implement the integral ln_hidden_layers\n",
    "        out_il = x\n",
    "        return out_il\n",
    "\n",
    "    def forward(self, x, input_grid):\n",
    "        batch_size = x.shape[0]\n",
    "        input_grid = input_grid.unsqueeze(0).repeat(batch_size, 1)\n",
    "        # Concatenate the grid to the input to gt the term v0\n",
    "        x = torch.cat((input_grid.unsqueeze(-1), x.unsqueeze(-1)), dim=-1)\n",
    "        # Apply the lifting transformation\n",
    "        x = self.R(x)\n",
    "        # Permute the axis, so that the axis corresponding to the physical space is the last (in order to compute the FFT)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Add padding (if input is non-periodic)\n",
    "        if self.padding != 0:\n",
    "            x = F.pad(x, [0, self.padding])\n",
    "\n",
    "        # Apply 3 integral layers\n",
    "        # TODO: apply three integral layers\n",
    "        x = x\n",
    "        x = x\n",
    "        x = x\n",
    "\n",
    "        # Remove padding if input is non-periodic\n",
    "        if self.padding != 0:\n",
    "            x = x[..., :-self.padding]\n",
    "\n",
    "        # Comeback to the origin position of the axes\n",
    "        # (batch_size, lifting_dimension, number of physical grid points) -> (batch_size, number of physical grid points, lifting_dimension)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply projection to go back to the original space (non-lifted)\n",
    "\n",
    "        x = self.Q(x).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modes = 4  # This variable corresponds to k_max\n",
    "width = 16 # This variable corresponds to d_v\n",
    "\n",
    "# Initialize the model\n",
    "fno = FNO1d(modes, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models Training\n",
    "\n",
    "Generally, the operator learning architecture $\\mathcal{N}_\\theta$ depends on a set of learnable parameter $\\theta \\in \\Theta$. For instance, in case of DeepONet, this set of parameters is the concatenation of the parameters of the trunk-net $\\theta_\\tau$ and branch-net $\\theta_\\beta$.  The model is trained by finding tuning parameters $\\theta$ such that a suitable loss function $J(\\theta)$ is minimized.\n",
    "\n",
    "$${\\rm Find}~\\theta^{\\ast} \\in \\Theta:\\quad \\theta^{\\ast} = {\\rm arg}\\min\\limits_{\\theta \\in \\Theta} \\left( J(\\theta)\\right).$$\n",
    "\n",
    "The loss function, can be chosen as the $L^1$ or $L^2$-difference between the neural network and the underlying target:\n",
    "\n",
    "$$ J(\\theta) = \\sum_{i}^M \\Big\\|u_i - \\mathcal{N}_\\theta(\\bar{u}_i)|\\Big\\|_{L^p}^p$$\n",
    "\n",
    "$p=1$ for $L^1$-loss and $p=2$ for $L^2$-loss squared.\n",
    "\n",
    "The optimization process is realized with the gradient descent (or more precisely with variants of the gradient descent such as Adam or SGD).\n",
    "\n",
    "The training steps can be defined by:\n",
    "\n",
    "1. Compute the loss function over the batch $j$: $J_S(\\theta)=\\sum_{(\\bar{u}_i, u_i) \\in {S}_j}^{n_{batch}} \\Big\\|u_i - \\mathcal{N}_\\theta(\\bar{u}_i)\\Big\\|_{L^p}^p$. Here, $n_{batch}$ in the total number of batches.\n",
    "\n",
    "2. Compute the gradient of $J_S(\\theta)$ with respect to the model parameters:  $\\nabla_\\theta J_S(\\theta)$\n",
    "\n",
    "3. Update the parameters according to the chosen optimizer, for instance for minibatch stochastic gradient descent $\\theta_{k+1} = \\theta_{k} - \\eta \\nabla_\\theta J_S(\\theta_{k}) $ with $k=1,...,(n_{epoch}\\cdot n_{batch})$ and $\\eta$ being the learning rate (argument $lr$ in the optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, training_set, testing_set, spatial_coordinates):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Use L1 (or mean absolute) loss (p=1) above\n",
    "    l = torch.nn.L1Loss()\n",
    "\n",
    "    with tqdm(total=epochs, unit=\"epoch\") as tepoch:\n",
    "        for epoch in range(epochs):\n",
    "            train_mse = 0.0\n",
    "            for step, (input_batch, output_batch) in enumerate(training_set):\n",
    "                optimizer.zero_grad()\n",
    "                # Compute model prediction for the input_batch\n",
    "                output_pred_batch = model(input_batch, spatial_coordinates)\n",
    "                # Compute loss. This computes directly the mean over samples, and the integral involved in the $L^p$ loss. Eventually, since output_pred_batch and output_batch are tensors of shape (Bxm), everything boils down to computing: mean(abs(output_pred_batch - output_batch)^p).\n",
    "                loss_f = l(output_pred_batch, output_batch)\n",
    "                # Compute gradients\n",
    "                loss_f.backward()\n",
    "                # Update the parameters\n",
    "                optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                train_mse += loss_f.item()\n",
    "            train_mse /= len(training_set)\n",
    "\n",
    "            # Compute test loss (it follows the same steps as above)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_relative_l2 = 0.0\n",
    "                for step, (input_batch, output_batch) in enumerate(testing_set):\n",
    "                    output_pred_batch = model(input_batch, spatial_coordinates)\n",
    "                    loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n",
    "                    test_relative_l2 += loss_f.item()\n",
    "                test_relative_l2 /= len(testing_set)\n",
    "\n",
    "            tepoch.set_postfix({'Train loss': train_mse, 'Learning Rate': scheduler.get_last_lr()[0], \"Test loss\": test_relative_l2})\n",
    "            tepoch.update(1)\n",
    "    tepoch.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot(model, testing_set, spatial_coordinates, samples=8):\n",
    "    rows = samples // 2\n",
    "    cols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(30, 18), dpi=100)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        k = 0\n",
    "        for step, (input_batch, output_batch) in enumerate(testing_set):\n",
    "            output_pred_batch = model(input_batch, spatial_coordinates)\n",
    "            for i in range(batch_size):\n",
    "                q = k // 2\n",
    "                mod = k % 2\n",
    "                axes[q, mod].grid(True, which=\"both\", ls=\":\")\n",
    "                axes[q, mod].plot(spatial_coordinates, output_batch[i], label=r'$u_T(x)$', color=\"C1\")\n",
    "                axes[q, mod].scatter(spatial_coordinates, output_pred_batch[i], label=r'$u^*_T(x)$', color=\"red\")\n",
    "                axes[q, mod].legend()\n",
    "                axes[q, mod].set_title(\"Sample \" + str(k))\n",
    "                k = k + 1\n",
    "                if k >= samples:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "step_size = 2000\n",
    "gamma = 0.99\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model and plot some test samples\n",
    "train(donet_random,\n",
    "      training_set=random_training_set_don,\n",
    "      testing_set=random_testing_set_don,\n",
    "      spatial_coordinates=grid_encoding_coordinates_donet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(donet_random,\n",
    "     testing_set=random_testing_set_don,\n",
    "     spatial_coordinates=grid_encoding_coordinates_donet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model and plot some test samples\n",
    "train(donet_grid,\n",
    "      training_set=grid_training_set_don,\n",
    "      testing_set=grid_testing_set_don,\n",
    "      spatial_coordinates=grid_encoding_coordinates_donet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(donet_grid,\n",
    "     testing_set=grid_testing_set_don,\n",
    "     spatial_coordinates=grid_encoding_coordinates_donet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model and plot some test samples\n",
    "train(donet_fourier,\n",
    "      training_set=fourier_training_set_don,\n",
    "      testing_set=fourier_testing_set_don,\n",
    "      spatial_coordinates=grid_encoding_coordinates_donet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(donet_fourier,\n",
    "     testing_set=fourier_testing_set_don,\n",
    "     spatial_coordinates=grid_encoding_coordinates_donet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model and plot some test samples\n",
    "train(fno,\n",
    "      training_set=training_set_fno,\n",
    "      testing_set=testing_set_fno,\n",
    "      spatial_coordinates=grid_encoding_coordinates_fno)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(fno,\n",
    "     testing_set=testing_set_fno,\n",
    "     spatial_coordinates=grid_encoding_coordinates_fno)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Operator\n",
    "\n",
    "[bogdanraonic3/ConvolutionalNeuralOperator](https://github.com/bogdanraonic3/ConvolutionalNeuralOperator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Inverse Operator\n",
    "[mroberto166/nio](https://github.com/mroberto166/nio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}